<h3 id="what-is-the-problem-that-is-being-solved">What is the problem that is being solved?</h3>
<p>Similar problem space to Megatron-LM, essentially we need to support training deep learning models with trillions of parameters. They point out early that the Megetron-LM model parallelism idea doesnâ€™t scale beyond one node since it depends on efficient communication between GPUs on the same server.</p>
<h3 id="what-are-the-key-results">What are the key results?</h3>
<p>The authors present ZeRO, a new optimizer for memory efficiency in large language models. They point out two types of states which consume memory:</p>
<ol type="1">
<li>Model states - optimizer states, gradients, parameters (things we probably want to keep around)</li>
<li>Residual states - temporary buffer and fragmented memory used during computation</li>
</ol>
<p>To optimize model states, ZeRO partitions everything which constitutes model state across parallel processes observing that not all model states are needed at each step of the model training. For residual states, a variety of strategies are employed including removing activiation replication, defining an appropriate size for temp buffers, and cleaning up fragmented memory.</p>
<p>Essentially, these optimizations allow ZeRO to parallelize training effectively across several nodes with linear increases in trainable model size as the number of nodes increases.</p>
<h3 id="what-are-some-of-the-limitations-and-how-might-this-work-be-improved">What are some of the limitations and how might this work be improved?</h3>
<p>This work is very specific to transformer networks. A more robust system would be able to automatically determine and exploit various opportunities for parallelism in various models. For example, a simple scheme would involve tracking dataflow through a model using dummy variables with versions. The transformations on these dummy variables could then be used to create a graph of dependencies, and by observing points in this graph where synchronization among nodes is not required, we could determine opportunities for parallelism.</p>
<h3 id="how-might-this-work-have-long-term-impact">How might this work have long term impact?</h3>
<p>This work is especially interesting now that GPT-3 has become available to researchers and performs much better than even the language models mentioned in this paper. This ongoing field of research obviously has a lot going on and it will be interesting to see what kinds of systems innovations result. This paper describes a modification which is highly specific to transformer and might not survive as models evolve. A system which can automatically determine model parallelism and optimize for it will be more useful.</p>
